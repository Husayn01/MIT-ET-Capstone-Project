{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFIKTLBDQHrS"
      },
      "source": [
        "# Lithological Classification: Deep Learning Approach\n",
        "\n",
        "## 1. Overview\n",
        "This notebook implements a **Feed-Forward Neural Network (Multi-Layer Perceptron)** to classify lithofacies from well log measurements using the FORCE 2020 dataset.\n",
        "\n",
        "### Workflow Differences from XGBoost\n",
        "- **Imputation:** Unlike tree-based models, Neural Networks cannot handle NaNs. We apply mean imputation.\n",
        "- **Scaling:** Input features are standardized (mean=0, std=1) to ensure stable gradient descent.\n",
        "- **Architecture:** We use a deep architecture with Batch Normalization and Dropout to prevent overfitting."
      ],
      "id": "yFIKTLBDQHrS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La7uHyOWQHrf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import gc\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 100)\n",
        "sns.set_style(\"ticks\")"
      ],
      "id": "La7uHyOWQHrf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14UtKAkzQHrl"
      },
      "source": [
        "## 2. Data Acquisition\n",
        "Loading the training and validation data from the source repository."
      ],
      "id": "14UtKAkzQHrl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rTIZajjQHrn",
        "outputId": "e3d10cce-7539-4776-d229-2925a6aa4f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Data Loaded in 19.97s\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading datasets...\")\n",
        "start_time = time.time()\n",
        "\n",
        "train = pd.read_csv('https://media.githubusercontent.com/media/Husayn01/Academic-Research/refs/heads/main/Data/train.csv', sep=';')\n",
        "test = pd.read_csv('https://media.githubusercontent.com/media/Husayn01/Academic-Research/refs/heads/main/Data/test_features.csv', sep=';')\n",
        "test_target = pd.read_csv('https://media.githubusercontent.com/media/Husayn01/Academic-Research/refs/heads/main/Data/test_target.csv', sep=';')\n",
        "\n",
        "# Merge test target\n",
        "test = test.merge(test_target, on=['WELL', 'DEPTH_MD'], how='left')\n",
        "\n",
        "print(f\"Data Loaded in {time.time() - start_time:.2f}s\")\n",
        "target_col = 'FORCE_2020_LITHOFACIES_LITHOLOGY'"
      ],
      "id": "0rTIZajjQHrn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIOTNi-GQHro"
      },
      "source": [
        "## 3. Feature Engineering\n",
        "Applying domain-specific transformations (Petrophysics) and spatial windowing."
      ],
      "id": "fIOTNi-GQHro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9K1c4NpQHrp",
        "outputId": "211659cc-f5b0-4f7e-8b34-3dec27839846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Engineering features...\n"
          ]
        }
      ],
      "source": [
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "    # Drop sparse/noisy columns\n",
        "    drop_cols = ['SGR', 'ROPA', 'RXO', 'MUDWEIGHT', 'DCAL', 'RMIC', 'FORCE_2020_LITHOFACIES_CONFIDENCE']\n",
        "    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    # Log transformations\n",
        "    for col in ['RDEP', 'RMED', 'RSHA']:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_LOG'] = np.log1p(df[col])\n",
        "\n",
        "    # Domain Ratios\n",
        "    if 'GR' in df.columns and 'RHOB' in df.columns:\n",
        "        df['GR_RHOB'] = df['GR'] / (df['RHOB'] + 0.001)\n",
        "\n",
        "    if 'NPHI' in df.columns and 'RHOB' in df.columns:\n",
        "        df['NPHI_RHOB_DIFF'] = df['NPHI'] - (2.65 - df['RHOB'])\n",
        "\n",
        "    if 'PEF' in df.columns:\n",
        "        df['PEF_SQ'] = df['PEF'] ** 2\n",
        "\n",
        "    # Fill Categoricals\n",
        "    if 'GROUP' in df.columns:\n",
        "        df['GROUP'] = df['GROUP'].fillna('Unknown')\n",
        "    if 'FORMATION' in df.columns:\n",
        "        df['FORMATION'] = df['FORMATION'].fillna('Unknown')\n",
        "\n",
        "    return df\n",
        "\n",
        "def vectorized_windowing(df, cols, window_size=1):\n",
        "    df_out = df.copy()\n",
        "    df_out = df_out.sort_values(['WELL', 'DEPTH_MD'])\n",
        "    for col in cols:\n",
        "        if col not in df_out.columns:\n",
        "            continue\n",
        "        for i in range(1, window_size + 1):\n",
        "            df_out[f'{col}_prev_{i}'] = df_out.groupby('WELL')[col].shift(i)\n",
        "            df_out[f'{col}_next_{i}'] = df_out.groupby('WELL')[col].shift(-i)\n",
        "        df_out[f'{col}_grad'] = df_out[col] - df_out.groupby('WELL')[col].shift(1)\n",
        "    return df_out\n",
        "\n",
        "print(\"Engineering features...\")\n",
        "train_eng = engineer_features(train)\n",
        "test_eng = engineer_features(test)\n",
        "\n",
        "aug_cols = ['GR', 'RHOB', 'NPHI', 'DTC', 'RDEP_LOG']\n",
        "train_final = vectorized_windowing(train_eng, aug_cols, window_size=1)\n",
        "test_final = vectorized_windowing(test_eng, aug_cols, window_size=1)"
      ],
      "id": "f9K1c4NpQHrp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVeBx8thQHrq"
      },
      "source": [
        "## 4. Preprocessing for Neural Networks\n",
        "1. **Encode Categoricals:** Convert text labels to integers.\n",
        "2. **Impute Missing Values:** Fill NaNs with mean (Crucial for NN).\n",
        "3. **Scale Data:** Standardize features to mean=0, std=1."
      ],
      "id": "LVeBx8thQHrq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym21P0nxQHrt",
        "outputId": "d505b9ea-c129-4a27-bdb6-0025fb6dd16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputing and Scaling data...\n",
            "Data ready for Neural Network.\n"
          ]
        }
      ],
      "source": [
        "# Encode Categoricals\n",
        "cat_cols = ['GROUP', 'FORMATION', 'WELL']\n",
        "for col in cat_cols:\n",
        "    if col in train_final.columns:\n",
        "        le = LabelEncoder()\n",
        "        unique_vals = pd.concat([train_final[col], test_final[col]]).astype(str).unique()\n",
        "        le.fit(unique_vals)\n",
        "        train_final[col] = le.transform(train_final[col].astype(str))\n",
        "        test_final[col] = le.transform(test_final[col].astype(str))\n",
        "\n",
        "# Encode Target\n",
        "le_target = LabelEncoder()\n",
        "train_final[target_col] = le_target.fit_transform(train_final[target_col])\n",
        "test_final[target_col] = le_target.transform(test_final[target_col])\n",
        "\n",
        "# Separate Features/Target\n",
        "drop_cols = ['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', target_col]\n",
        "X = train_final.drop(columns=drop_cols)\n",
        "y = train_final[target_col]\n",
        "X_test_data = test_final.drop(columns=drop_cols)\n",
        "y_test_data = test_final[target_col]\n",
        "\n",
        "# Clean column names\n",
        "X.columns = [c.replace('[', '_').replace(']', '_') for c in X.columns]\n",
        "X_test_data.columns = [c.replace('[', '_').replace(']', '_') for c in X_test_data.columns]\n",
        "\n",
        "# IMPUTATION & SCALING\n",
        "print(\"Imputing and Scaling data...\")\n",
        "# Align columns\n",
        "X_test_data = X_test_data[X.columns]\n",
        "\n",
        "# Impute\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "X_test_imputed = imputer.transform(X_test_data)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "print(\"Data ready for Neural Network.\")"
      ],
      "id": "Ym21P0nxQHrt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgIXE1TOQHrv"
      },
      "source": [
        "## 5. Model Definition and Training\n",
        "We use a 3-layer MLP with Batch Normalization and Dropout for regularization."
      ],
      "id": "UgIXE1TOQHrv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jZ7sgwsQHrw",
        "outputId": "a6531a6b-d61e-4772-9909-ceb78bb49750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting 10-Fold Cross-Validation...\n",
            "   Fold 1/10 | Accuracy: 0.8949 | Epochs: 33\n",
            "   Fold 2/10 | Accuracy: 0.8864 | Epochs: 16\n",
            "   Fold 3/10 | Accuracy: 0.8958 | Epochs: 40\n",
            "   Fold 4/10 | Accuracy: 0.8945 | Epochs: 28\n",
            "   Fold 5/10 | Accuracy: 0.8957 | Epochs: 38\n",
            "   Fold 6/10 | Accuracy: 0.8921 | Epochs: 32\n",
            "   Fold 7/10 | Accuracy: 0.8997 | Epochs: 50\n",
            "   Fold 8/10 | Accuracy: 0.8909 | Epochs: 23\n",
            "   Fold 9/10 | Accuracy: 0.8933 | Epochs: 40\n",
            "   Fold 10/10 | Accuracy: 0.8950 | Epochs: 27\n",
            "========================================\n",
            "Average CV Accuracy: 0.8938\n",
            "Total CV Time: 20.93 minutes\n"
          ]
        }
      ],
      "source": [
        "def build_model(input_dim, num_classes):\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=(input_dim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "folds = 10\n",
        "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 50\n",
        "test_preds_nn = np.zeros((len(X_test_scaled), 12))\n",
        "scores_nn = []\n",
        "\n",
        "print(f\"Starting {folds}-Fold Cross-Validation...\")\n",
        "cv_start = time.time()\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y)):\n",
        "    fold_start = time.time()\n",
        "    X_train_fold, y_train_fold = X_scaled[train_idx], y[train_idx]\n",
        "    X_val_fold, y_val_fold = X_scaled[val_idx], y[val_idx]\n",
        "\n",
        "    model = build_model(X_scaled.shape[1], 12)\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_fold, y_train_fold,\n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    val_probs = model.predict(X_val_fold, batch_size=BATCH_SIZE, verbose=0)\n",
        "    test_probs = model.predict(X_test_scaled, batch_size=BATCH_SIZE, verbose=0)\n",
        "    test_preds_nn += test_probs / folds\n",
        "\n",
        "    acc = accuracy_score(y_val_fold, np.argmax(val_probs, axis=1))\n",
        "    scores_nn.append(acc)\n",
        "\n",
        "    print(f\"   Fold {fold+1}/{folds} | Accuracy: {acc:.4f} | Epochs: {len(history.history['loss'])}\")\n",
        "\n",
        "    del model, X_train_fold, X_val_fold\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"=\"*40)\n",
        "print(f\"Average CV Accuracy: {np.mean(scores_nn):.4f}\")\n",
        "print(f\"Total CV Time: {(time.time() - cv_start)/60:.2f} minutes\")"
      ],
      "id": "5jZ7sgwsQHrw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhHceoMAQHrx"
      },
      "source": [
        "## 6. Evaluation\n",
        "Assessing model performance on the blind test set."
      ],
      "id": "mhHceoMAQHrx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak6QwyuNQHry",
        "outputId": "d594dea5-7bbe-4e4d-b26a-beb5bbe3eb15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "      Sandstone       0.76      0.81      0.79     24048\n",
            "          Shale       0.84      0.85      0.85     83975\n",
            "Sandstone/Shale       0.38      0.32      0.35     17558\n",
            "      Limestone       0.38      0.47      0.42      4798\n",
            "          Chalk       0.01      0.01      0.01       625\n",
            "       Dolomite       0.00      0.00      0.00       416\n",
            "           Marl       0.13      0.10      0.11      3306\n",
            "      Anhydrite       1.00      0.29      0.45       125\n",
            "         Halite       0.00      0.00      0.00         0\n",
            "           Coal       0.83      0.59      0.69       690\n",
            "       Basement       0.00      0.00      0.00         0\n",
            "           Tuff       0.77      0.73      0.75      1245\n",
            "\n",
            "       accuracy                           0.74    136786\n",
            "      macro avg       0.43      0.35      0.37    136786\n",
            "   weighted avg       0.73      0.74      0.73    136786\n",
            "\n"
          ]
        }
      ],
      "source": [
        "final_test_labels = np.argmax(test_preds_nn, axis=1)\n",
        "lithology_keys = {30000: 'Sandstone', 65030: 'Sandstone/Shale', 65000: 'Shale',\n",
        "                  80000: 'Marl', 74000: 'Dolomite', 70000: 'Limestone',\n",
        "                  70032: 'Chalk', 88000: 'Halite', 86000: 'Anhydrite',\n",
        "                  99000: 'Tuff', 90000: 'Coal', 93000: 'Basement'}\n",
        "target_names_mapped = [lithology_keys.get(code, str(code)) for code in le_target.classes_]\n",
        "all_labels = np.arange(len(le_target.classes_))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    y_test_data,\n",
        "    final_test_labels,\n",
        "    labels=all_labels,\n",
        "    target_names=target_names_mapped,\n",
        "    zero_division=0\n",
        "))"
      ],
      "id": "Ak6QwyuNQHry"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}